{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0, 0, 2]], 4], [[[0, 0, 1]], 1], [[[0, 0, 3]], 5], [[[0, 0, 4]], 7], [[[0, 3, 1]], 3]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Activation, Embedding, Conv1D, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from dataprocessing.find_top_movie_with_sequence import find_top_dataset\n",
    "from dataprocessing.label_and_feature import getSortedClassContents, checkConsists, buildFeature, saveClassesToFile\n",
    "from dataprocessing.time import checkingTimeDifferent\n",
    "from recom_model import trainWithBidirectional,trainWithEmbeddingDense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_CONTENTS_ON_USER = 250\n",
    "MAX_DAYS = 7\n",
    "MAX_SEQUENCE = 1\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "recommandation_df = pd.read_csv('{}/data1.csv'.format(DATA_DIR)).sort_values(by=['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 15, 23, 39, 69, 79, 162, 169, 202, 205, 213, 252, 255, 306, 308, 312, 322, 369, 371, 379, 434, 448, 455, 478, 479, 480, 481, 492, 507, 513, 535, 548, 623, 626, 640, 644, 657, 682, 683, 697, 700, 752, 772, 776, 777, 781, 824, 832, 839, 883, 913, 936, 960, 964, 977, 989, 993, 1006, 1020, 1026, 1115, 1137, 1185, 1187, 1193, 1239, 1242, 1256, 1269, 1304, 1310, 1397, 1415, 1420, 1428, 1429, 1433, 1436, 1437, 1490, 1498, 1506, 1512, 1521, 1534, 1554, 1559, 1567, 1578, 1579, 1589, 1591, 1599, 1602, 1606, 1607, 1609, 1619, 1620, 1624, 1641, 1652, 1659, 1677, 1700, 1716, 1719, 1756, 1758, 1774, 1777, 1781, 1788, 1790, 1791, 1795, 1798, 1806, 1807, 1813, 1819, 1823, 1826, 1830, 1833, 1834, 1836, 1838, 1839, 1840, 1845, 1847, 1853, 1855, 1862, 1863, 1872, 1873, 1876, 1877, 1879, 1885, 1896, 1910, 1912, 1913, 1927, 1928, 1935, 1936]\n",
      "151\n"
     ]
    }
   ],
   "source": [
    "sortedClassContents = getSortedClassContents(recommandation_df, MIN_CONTENTS_ON_USER)\n",
    "saveClassesToFile(sortedClassContents)\n",
    "lenSortedClassContents = len(sortedClassContents)\n",
    "print(lenSortedClassContents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveDatas(datas):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for feature, label in datas:\n",
    "        features.append(feature)\n",
    "        labels.append(label)\n",
    "\n",
    "    a_file = open('{}/backup_features.txt'.format(DATA_DIR), \"w\")\n",
    "    for row in features:\n",
    "        np.savetxt(a_file, row[0])\n",
    "    a_file.close()\n",
    "\n",
    "    a_file = open('{}/backup_labels.txt'.format(DATA_DIR), \"w\")\n",
    "    np.savetxt(a_file, labels)\n",
    "    a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_data(data_frame):\n",
    "    # datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "\n",
    "    print(\"start =\", now)\n",
    "\n",
    "    visitors_df = data_frame['visitor'].drop_duplicates()\n",
    "    maxItem = data_frame['Content'].max()\n",
    "    training_data = []\n",
    "    for index, item in visitors_df.iteritems():\n",
    "        video = data_frame[data_frame['visitor'] == item]\n",
    "        if video.size > 1:\n",
    "            tempContents = []\n",
    "            indexContent = 0\n",
    "            for index, item in video['Content'].iteritems():\n",
    "                if item not in sortedClassContents:\n",
    "                    continue\n",
    "                if len(tempContents) > MAX_SEQUENCE:\n",
    "                    tempContents = tempContents[1:]\n",
    "                    continue\n",
    "                if checkConsists(item, tempContents):\n",
    "                    continue\n",
    "                if len(tempContents) > 0:\n",
    "                    feature = buildFeature(tempContents, sortedClassContents, MAX_SEQUENCE)\n",
    "                    label = sortedClassContents.index(item)\n",
    "                    training_data.append([[feature], label])\n",
    "                tempContents.append(item)\n",
    "                indexContent += 1\n",
    "\n",
    "    now = datetime.now()\n",
    "    print(\"end =\", now)\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start = 2020-11-16 10:07:44.667945\n",
      "end = 2020-11-16 10:37:31.659633\n",
      "17406\n"
     ]
    }
   ],
   "source": [
    "data1 = pd.read_csv('{}/data1.csv'.format(DATA_DIR)).sort_values(by=['time'])\n",
    "training_data = build_training_data(data1)\n",
    "print(len(training_data))\n",
    "saveDatas(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('{}/data2.csv'.format(DATA_DIR)).sort_values(by=['time'])\n",
    "training_data = np.concatenate((training_data, build_training_data(data2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{}.before', len(training_data))\n",
    "data3 = pd.read_csv('{}/data3.csv'.format(DATA_DIR)).sort_values(by=['time'])\n",
    "training_data = np.concatenate((training_data, build_training_data(data3)))\n",
    "print(len(training_data))\n",
    "saveDatas(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4 = pd.read_csv('{}/data4.csv'.format(DATA_DIR)).sort_values(by=['time'])\n",
    "training_data = np.concatenate((training_data, build_training_data(data4)))\n",
    "print(len(training_data))\n",
    "saveDatas(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5 = pd.read_csv('{}/data5.csv'.format(DATA_DIR)).sort_values(by=['time'])\n",
    "training_data = np.concatenate((training_data, build_training_data(data5)))\n",
    "print(len(training_data))\n",
    "saveDatas(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data6 = pd.read_csv('{}/data6.csv'.format(DATA_DIR)).sort_values(by=['time'])\n",
    "training_data = np.concatenate((training_data, build_training_data(data6)))\n",
    "print(len(training_data))\n",
    "saveDatas(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = find_top_dataset(training_data)\n",
    "print(\"training size: \", len(training_data))\n",
    "print(\"validation size: \", len(validation_data))\n",
    "# print(training_data)\n",
    "print(\"-------------------------\")\n",
    "# print(validation_data)\n",
    "random.shuffle(training_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "labels = []\n",
    "for feature, label in training_data:\n",
    "    features.append(feature)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(features)\n",
    "Y = np.array(labels)\n",
    "print(X.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_val = []\n",
    "labels_val = []\n",
    "for feature, label in validation_data:\n",
    "    features_val.append(feature)\n",
    "    labels_val.append(label)\n",
    "\n",
    "X_val = np.array(features_val)\n",
    "Y_val = np.array(labels_val).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cnn\n",
    "X = np.squeeze(X, axis=1)\n",
    "print(X.shape)\n",
    "model = Sequential()   \n",
    "model.add(Dense(1024, activation=\"relu\", input_shape=(X.shape[1:])))\n",
    "model.add(Dense(1024, activation=\"relu\"))\n",
    "model.add(Dense(1024, activation=\"relu\"))\n",
    "model.add(Dense(1024, activation=\"relu\"))\n",
    "model.add(Dense(1024, activation=\"relu\"))\n",
    "model.add(Dense(1024, activation=\"relu\"))\n",
    "model.add(Dense(lenSortedClassContents))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for embedding Dense\n",
    "X = np.squeeze(X, axis=1)\n",
    "model = trainWithEmbeddingDense(MAX_SEQUENCE, lenSortedClassContents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "name = \"testing-1{}\".format(now)\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(name))\n",
    "history = model.fit(X, Y, epochs=256, batch_size=64, verbose=1,validation_data=(X_val, Y_val), callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history, 'accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history, 'val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history, 'val_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get5TopPredict(predict):\n",
    "    predicts = predict[0].argsort()[-5:][::-1]\n",
    "    for i in predicts:\n",
    "        print(sortedClassContents[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "def predictDense(first):\n",
    "    predict = model.predict([[sortedClassContents.index(first)]])\n",
    "    get5TopPredict(predict)\n",
    "    return sortedClassContents[np.argmax(predict[0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictResult = predictDense(1506)\n",
    "print(predictResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model = tf.function(lambda x: model(x))\n",
    "# This is important, let's fix the input size.\n",
    "concrete_func = run_model.get_concrete_function(\n",
    "    tf.TensorSpec([1,6], model.inputs[0].dtype))\n",
    "\n",
    "# model directory.\n",
    "MODEL_DIR = \"model\"\n",
    "model.save(MODEL_DIR, save_format=\"tf\", signatures=concrete_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"model\"\n",
    "model.save(MODEL_DIR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model with TensorFlow to get expected results.\n",
    "TEST_CASES = 10\n",
    "\n",
    "# Run the model with TensorFlow Lite\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "print(input_details)\n",
    "print(output_details)\n",
    "print(input_details[0][\"index\"])\n",
    "for i in range(TEST_CASES):\n",
    "  expected = model.predict([[i]])\n",
    "  interpreter.set_tensor( input_details[0][\"index\"], np.array([[i]]).astype(np.float32))\n",
    "  interpreter.invoke()\n",
    "  result = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "\n",
    "  # Assert if the result of TFLite model is consistent with the TF model.\n",
    "  np.testing.assert_almost_equal(expected, result)\n",
    "  print(\"Done. The result of TensorFlow matches the result of TensorFlow Lite.\")\n",
    "\n",
    "  # Please note: TfLite fused Lstm kernel is stateful, so we need to reset\n",
    "  # the states.\n",
    "  # Clean up internal states.\n",
    "  interpreter.reset_all_variables()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(\"tf_model/converted_model.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.set_tensor( input_details[0][\"index\"], np.array([[sortedClassContents.index(700)]]).astype(np.float32))\n",
    "interpreter.invoke()\n",
    "result = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "print(sortedClassContents[np.argmax(result)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
